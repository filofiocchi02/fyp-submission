{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53d3a8f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\n  File \"C:\\Users\\filip\\anaconda3\\lib\\imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow_internal'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow_internal\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mfind_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ERR_MSG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named '_pywrap_tensorflow_internal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '_pywrap_tensorflow_internal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0d7aba641247>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAttention\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomponent_api_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 74\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\n  File \"C:\\Users\\filip\\anaconda3\\lib\\imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow_internal'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow_internal\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, Matern, RationalQuadratic, ExpSineSquared\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "from tensorflow.keras.layers import Attention, Input, Conv1D, MaxPooling1D, LSTM, Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0f48a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at C:\\Users\\filip\\anaconda3:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "tensorflow                1.12.0                   pypi_0    pypi\n",
      "tensorflow-datasets       4.9.2                    pypi_0    pypi\n",
      "tensorflow-estimator      2.9.0            py38haa95532_0  \n",
      "tensorflow-io-gcs-filesystem 0.28.0                   pypi_0    pypi\n",
      "tensorflow-metadata       1.14.0                   pypi_0    pypi\n",
      "tensorflow-probability    0.21.0                   pypi_0    pypi\n"
     ]
    }
   ],
   "source": [
    "!conda list tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf7c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.5\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fb8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install tensorflow==1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8dc1316",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\n  File \"C:\\Users\\filip\\anaconda3\\lib\\imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow_internal'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow_internal\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mfind_module\u001b[1;34m(name, path)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ERR_MSG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named '_pywrap_tensorflow_internal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named '_pywrap_tensorflow_internal'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-76e820d8846e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0msilence_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomponent_api_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 74\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\n  File \"C:\\Users\\filip\\anaconda3\\lib\\imp.py\", line 296, in find_module\n    raise ImportError(_ERR_MSG.format(name), name=name)\nImportError: No module named '_pywrap_tensorflow_internal'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\filip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\n    import _pywrap_tensorflow_internal\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "spec = importlib.util.find_spec('silence_tensorflow')\n",
    "if spec is not None:\n",
    "    import silence_tensorflow.auto\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os \n",
    "import gpflow\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "  \n",
    "    \n",
    "def load_dataset_train_test_split(test_size = 0.1, split_id: int = 1, print_dataset: bool=False):\n",
    "    file_path = 'elevators_all.npy'\n",
    "    data_all = np.load(file_path, allow_pickle=True)\n",
    "    x_all, y_all = data_all[()]['x_all'], data_all[()]['y_all']\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=test_size, random_state=split_id)\n",
    "    if print_dataset:\n",
    "        print('*** Dataset: {} ****' .format('elevators'))\n",
    "        print('N_train: {}   N_test: {}   D: {} \\n' .format(y_train.size, y_test.size, x_train.shape[1]))\n",
    "           \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    y_train_mean, y_train_std = y_train.mean(), y_train.std()\n",
    "    y_train = (y_train - y_train_mean)/y_train_std\n",
    "    y_test = (y_test - y_train_mean)/y_train_std\n",
    "        \n",
    "    return x_train, x_test, y_train, y_test \n",
    " \n",
    "    \n",
    "def compute_rmse_nlpd(y_true, y_pred, var_test):\n",
    "    mse_term = np.square(y_true - y_pred)\n",
    "    nlpd = 0.5*(np.log(2.*np.pi*var_test) + mse_term/var_test)\n",
    "    rmse = np.sqrt(mse_term.mean())\n",
    "\n",
    "    return rmse, nlpd.mean()\n",
    "    \n",
    "    \n",
    "# Get flags from the command line\n",
    "def get_flags():\n",
    "    flags = tf.compat.v1.flags\n",
    "    FLAGS = flags.FLAGS\n",
    "    flags.DEFINE_integer('batch_size', 2048, 'Batch size')\n",
    "    flags.DEFINE_integer('num_epochs', 400, 'Number of epochs used to train DMGP/DFGP model')\n",
    "    flags.DEFINE_integer('display_freq', 10, 'Display loss function value every FLAGS.display_freq epochs')\n",
    "    flags.DEFINE_integer('num_splits', 1, 'Number of random data splits used - number of experiments run for a model')\n",
    "    flags.DEFINE_integer('rank', 100, 'Rank r for MGP, FGP, SGPR')\n",
    "    flags.DEFINE_integer('d_mgp', 5, 'Number of output dimensions for MGP\\'s projection')\n",
    "    flags.DEFINE_string('dataset', 'elevators', 'Dataset name')\n",
    "\n",
    "    return FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5b8a62e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset_train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e697c34f21cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset_train_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprint_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load_dataset_train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = load_dataset_train_test_split(print_dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdef561",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_train), '\\n', x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e77a468",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8df95c941fc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(y_train), '\\n', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16ff5cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen rank: 100 \n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "#####   MGP    #####\n",
      "\n",
      "*** Split: 0 ***\n",
      "Training...\n",
      "Tensor(\"Cholesky:0\", shape=(100, 100), dtype=float64)\n",
      "Tensor(\"strided_slice_2:0\", shape=(100, 1), dtype=float64)\n",
      "Tensor(\"Cholesky:0\", shape=(100, 100), dtype=float64)\n",
      "Tensor(\"strided_slice_2:0\", shape=(100, 1), dtype=float64)\n",
      "Tensor(\"Cholesky:0\", shape=(100, 100), dtype=float64)\n",
      "Tensor(\"strided_slice_2:0\", shape=(100, 1), dtype=float64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(5, 5) dtype=float64>),).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-0922f1266c5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;31m# Run the MGP model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m \u001b[0mrun_mgp_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-0922f1266c5d>\u001b[0m in \u001b[0;36mrun_mgp_model\u001b[1;34m(batch_size, num_epochs, display_freq, num_splits, rank, d_mgp)\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0moptimization_step_dnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimization_step_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdisplay_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch: {}/{}   log-marginal-lkl: {:.3f}'\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-0922f1266c5d>\u001b[0m in \u001b[0;36moptimization_step_kernel\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_mgp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneg_log_marginal_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_trainable_vars_kernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0madam_opt_kernel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_trainable_vars_kernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mcalled\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcross\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mreplica\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m     \"\"\"\n\u001b[1;32m--> 640\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_empty_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     71\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mvariable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     raise ValueError(f\"No gradients provided for any variable: {variable}. \"\n\u001b[0m\u001b[0;32m     74\u001b[0m                      f\"Provided `grads_and_vars` is {grads_and_vars}.\")\n\u001b[0;32m     75\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(5, 5) dtype=float64>),)."
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "spec = importlib.util.find_spec('silence_tensorflow')\n",
    "if spec is not None:\n",
    "    import silence_tensorflow.auto\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "from gpflow import set_trainable\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "def run_mgp_model(batch_size = 2048, num_epochs = 400, display_freq = 10, num_splits = 1, rank = 100, d_mgp = 5):     \n",
    "    print('\\n#####   MGP    #####')\n",
    "    \n",
    "    # Define arrays to store values of RMSE, NLPD, and training time for each split\n",
    "    rmse_vec = np.zeros(num_splits)\n",
    "    nlpd_vec = np.zeros(num_splits)\n",
    "    train_time_vec = np.zeros(num_splits)\n",
    "    \n",
    "    for split_id in range(num_splits): \n",
    "        # for reproducibility    \n",
    "        tf.random.set_seed(1) \n",
    "        np.random.seed(1) \n",
    "        \n",
    "        # Load standardized dataset\n",
    "        x_train, x_test, y_train, y_test = load_dataset_train_test_split(test_size = 0.1, split_id=split_id)\n",
    "\n",
    "        print('\\n*** Split: %d ***' %split_id)\n",
    "        \n",
    "        # Define model\n",
    "        model_mgp = MGP_model(data=(x_train, y_train), rank=rank, dnn_out=d_mgp, eps_sq=.1, sigma_n_sq=.1)\n",
    "\n",
    "        # Split parameters to those corresponding to kernel and DNN\n",
    "        list_trainable_vars_kernel = tuple([var for var in model_mgp.trainable_variables if var.name == 'Variable:0'])\n",
    "        list_trainable_vars_dnn = tuple([var for var in model_mgp.trainable_variables if var.name != 'Variable:0'])\n",
    "\n",
    "        # Set minibatch size\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(buffer_size=y_train.size)\n",
    "        batches_iter = iter(train_dataset.batch(batch_size))\n",
    "        iters_per_epoch = y_train.size // batch_size\n",
    "        \n",
    "        # Define learning rates for kernel and DNN parameters separately\n",
    "        lrate_kernel = 0.1\n",
    "        lrate_dnn = 0.1\n",
    "        \n",
    "        # Define Adam optimizers for kernel and DNN parameters separately\n",
    "        adam_opt_kernel = tf.optimizers.Adam(learning_rate=lrate_kernel)\n",
    "        adam_opt_dnn = tf.optimizers.Adam(learning_rate=lrate_dnn)\n",
    "\n",
    "        # Differentiation of negative log marginal likelihood with respect to kernel parameters\n",
    "        @tf.function(autograph=False)\n",
    "        def optimization_step_kernel():\n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                tape.watch(list_trainable_vars_kernel)\n",
    "                objective = model_mgp.neg_log_marginal_likelihood()\n",
    "                grads = tape.gradient(objective, list_trainable_vars_kernel)\n",
    "            adam_opt_kernel.apply_gradients(zip(grads, list_trainable_vars_kernel))\n",
    "            return objective\n",
    "            \n",
    "        # Differentiation of negative log marginal likelihood with respect to DNN weights\n",
    "        @tf.function(autograph=False)\n",
    "        def optimization_step_dnn():\n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                tape.watch(list_trainable_vars_dnn)\n",
    "                objective = model_mgp.neg_log_marginal_likelihood(next(batches_iter))\n",
    "                grads = tape.gradient(objective, list_trainable_vars_dnn)\n",
    "            adam_opt_dnn.apply_gradients(zip(grads, list_trainable_vars_dnn))\n",
    "            return objective\n",
    "            \n",
    "        print('Training...')\n",
    "        start_time = datetime.now() \n",
    "        # Training\n",
    "        for epoch in range(num_epochs):     \n",
    "            for _ in range(iters_per_epoch):\n",
    "                optimization_step_dnn()\n",
    "            \n",
    "            loss_value = optimization_step_kernel()\n",
    "            if (epoch+1) % display_freq == 0 or epoch==0:\n",
    "                print('Epoch: {}/{}   log-marginal-lkl: {:.3f}' .format(epoch+1, num_epochs, -loss_value.numpy()))\n",
    "        train_time = datetime.now() - start_time\n",
    "        print('Done')\n",
    "        \n",
    "        # Prediction\n",
    "        f_mean, f_var = model_mgp.predict_y(x_test)\n",
    "        f_mean, f_var = f_mean.numpy(), f_var.numpy()\n",
    "        \n",
    "        # RMSE and NLPD computation\n",
    "        rmse_test, nlpd_test = compute_rmse_nlpd(y_test, f_mean, f_var)\n",
    "        rmse_vec[split_id], nlpd_vec[split_id], train_time_vec[split_id] = rmse_test, nlpd_test, train_time.total_seconds()\n",
    "\n",
    "        print('\\nMGP  RMSE: %.3f    NLPD: %.3f' % (rmse_test, nlpd_test))  \n",
    "        print('Training time  Full: {}  or {:.3f} seconds' .format(train_time, train_time.total_seconds()))\n",
    "            \n",
    "    print('\\n\\nDMGP-Averaged   RMSE: %.3f +/- %.3f   NLPD: %.3f +/- %.3f' % (rmse_vec.mean(), rmse_vec.std(), nlpd_vec.mean(), nlpd_vec.std()))  \n",
    "    print('Average training time  %.3f +/- %.3f seconds' %(train_time_vec.mean(), train_time_vec.std()))\n",
    "    print()\n",
    "    print(60*'=')\n",
    "    \n",
    "\n",
    "print('Chosen rank: {} \\n' .format(100))\n",
    "print(60*'=')\n",
    "print()\n",
    "\n",
    "# Run the MGP model\n",
    "run_mgp_model()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4709fe46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e53199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7441d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b7ffd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Cleaned_data.pkl'\n",
    "df = pd.read_pickle(file_path)#.sample(n=10_000_000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4b90b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features considered\n",
    "features = [\n",
    "'Wind.speed.me',\n",
    "'Front.bearing.temp.me',\n",
    "'Rear.bearing.temp.me',\n",
    "'Stator1.temp.me',\n",
    "'Nacelle.ambient.temp.me',\n",
    "'Nacelle.temp.me',\n",
    "'Transformer.temp.me',\n",
    "'Gear.oil.temp.me',\n",
    "'Gear.oil.temp.me.1',\n",
    "'Top.box.temp.me',\n",
    "'Hub.temp.me',\n",
    "'Conv.Amb.temp.me',\n",
    "'Rotor.bearing.temp.me',\n",
    "'Transformer.cell.temp.me',\n",
    "'Motor.axis1.temp.me',\n",
    "'Motor.axis2.temp.me',\n",
    "'Motor.axis3.temp.me',\n",
    "'CPU.temp.me',\n",
    "'Rotor.speed.me',\n",
    "'Blade.ang.pitch.pos.A.me',\n",
    "'Blade.ang.pitch.pos.B.me',\n",
    "'Blade.ang.pitch.pos.C.me',\n",
    "'Gear.oil.inlet.press.me',\n",
    "'Gear.oil.pump.press.me',\n",
    "'Drive.train.acceleration.me',\n",
    "'Tower.Acceleration.x',\n",
    "'Tower.Acceleration.y'\n",
    "]\n",
    "output_feature = 'Power.me'\n",
    "\n",
    "# Only consider the first turbine\n",
    "df = df[df['turbine'] == 1]\n",
    "\n",
    "df = df.dropna(subset=features + [output_feature])\n",
    "df = df.sample(n=100000, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "393874a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "num_epochs = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b68f432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 110)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "002d22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_train_test_split():\n",
    "    # Dataset to train the autoencoder (full dataset)\n",
    "    X = df[features]\n",
    "    y = df[output_feature]\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Standardize both X and y\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Standardize y (target variable)\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "    y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2289b50",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_dataset_train_test_split() got an unexpected keyword argument 'print_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-e697c34f21cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset_train_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprint_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: load_dataset_train_test_split() got an unexpected keyword argument 'print_dataset'"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = load_dataset_train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02d61caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "spec = importlib.util.find_spec('silence_tensorflow')\n",
    "if spec is not None:\n",
    "    import silence_tensorflow.auto\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "from gpflow import set_trainable\n",
    "import numpy as np\n",
    "# from helper import *\n",
    "# from models import MGP_model, FGP_model\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d4949b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Aristeidis Panos\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import importlib.util\n",
    "spec = importlib.util.find_spec('silence_tensorflow')\n",
    "if spec is not None:\n",
    "    import silence_tensorflow.auto\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os \n",
    "import gpflow\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "    \n",
    "def compute_rmse_nlpd(y_true, y_pred, var_test):\n",
    "    mse_term = np.square(y_true - y_pred)\n",
    "    nlpd = 0.5*(np.log(2.*np.pi*var_test) + mse_term/var_test)\n",
    "    rmse = np.sqrt(mse_term.mean())\n",
    "\n",
    "    return rmse, nlpd.mean()\n",
    "    \n",
    "    \n",
    "# Get flags from the command line\n",
    "def get_flags():\n",
    "    flags = tf.compat.v1.flags\n",
    "    FLAGS = flags.FLAGS\n",
    "    flags.DEFINE_integer('batch_size', 2048, 'Batch size')\n",
    "    flags.DEFINE_integer('num_epochs', 400, 'Number of epochs used to train DMGP/DFGP model')\n",
    "    flags.DEFINE_integer('display_freq', 10, 'Display loss function value every FLAGS.display_freq epochs')\n",
    "    flags.DEFINE_integer('num_splits', 1, 'Number of random data splits used - number of experiments run for a model')\n",
    "    flags.DEFINE_integer('rank', 100, 'Rank r for MGP, FGP, SGPR')\n",
    "    flags.DEFINE_integer('d_mgp', 5, 'Number of output dimensions for MGP\\'s projection')\n",
    "    flags.DEFINE_string('dataset', 'elevators', 'Dataset name')\n",
    "\n",
    "    return FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e5b1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, TypeVar\n",
    "import importlib.util\n",
    "spec = importlib.util.find_spec('silence_tensorflow')\n",
    "if spec is not None:\n",
    "    import silence_tensorflow.auto\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K_bd\n",
    "from itertools import product\n",
    "import os \n",
    "from gpflow.base import Module, Parameter\n",
    "from gpflow.config import default_float, set_default_float\n",
    "from gpflow.utilities import ops, positive\n",
    "from scipy import special\n",
    "\n",
    "class MGP_model(Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 data: Tuple[tf.Tensor, tf.Tensor],\n",
    "                 rank: int = 20,\n",
    "                 dnn_out: int = 2,\n",
    "                 eps_sq: float = 1,\n",
    "                 sigma_n_sq: float = 1,\n",
    "                 sigma_f_sq: float = 1):\n",
    "\n",
    "        if data[1].dtype == np.float64:\n",
    "            K_bd.set_floatx('float64')\n",
    "        else:\n",
    "            set_default_float(np.float32)\n",
    "\n",
    "        self.num_data = tf.cast(data[1].shape[0], default_float())\n",
    "        self.data = (tf.cast(tf.squeeze(data[0]), default_float()), tf.cast(data[1], default_float()))\n",
    "\n",
    "        self.dim = dnn_out\n",
    "        self.sqrt_2 = tf.cast(np.sqrt(2), default_float())\n",
    "        self.minus_ln_2 = tf.cast(-1*np.log(2), default_float())\n",
    "\n",
    "        num_eig_fun = 7\n",
    "        all_tupples = np.asarray(list(product(range(1, num_eig_fun+1), repeat=self.dim)))\n",
    "        args_tupples = np.argsort(all_tupples.sum(1))\n",
    "        all_tupples = all_tupples[args_tupples]\n",
    "        tupples = all_tupples[:rank]\n",
    "\n",
    "        sum_tupples_D = tupples.sum(1) - self.dim\n",
    "        self.sum_tupples_D = tf.cast(sum_tupples_D, default_float())\n",
    "\n",
    "        self.tupples_1 = tf.cast(tupples - 1, tf.int32)\n",
    "        self.max_rank = tupples.max()\n",
    "        self.dim_half = 0.5 * self.dim\n",
    "\n",
    "        self.this_range_1_2 = tf.range(self.max_rank, dtype=default_float())\n",
    "        self.tf_range_D = tf.range(self.dim)\n",
    "        self.gamma_term = np.exp(-0.5 * (special.gammaln(tupples).sum(1) + sum_tupples_D * np.log(2)))\n",
    "\n",
    "        self.eye_k = tf.eye(rank, dtype=default_float())\n",
    "        self.yTy = tf.reduce_sum(tf.math.square(self.data[1]))\n",
    "        self.coeff_n_tf = tf.constant(np.load('hermite_coeff.npy')[:self.max_rank, :self.max_rank], dtype=default_float())\n",
    "\n",
    "        self.rotation = Parameter(np.eye(self.dim), dtype=default_float())\n",
    "        self.eps_sq = Parameter(eps_sq, transform=positive(), dtype=default_float(), trainable=True)\n",
    "        self.sigma_f_sq = 1. #Parameter(sigma_f_sq, transform=positive(), dtype=default_float(), trainable=True)\n",
    "        self.sigma_n_sq = Parameter(sigma_n_sq, transform=positive(), dtype=default_float(), trainable=True)\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(dnn_out, activation='linear', input_dim=data[0].shape[1]))\n",
    "        self.neural_net = model\n",
    "\n",
    "\n",
    "    def neg_log_marginal_likelihood(self, data_in: tf.Tensor=None) -> tf.Tensor:\n",
    "        if data_in is None:\n",
    "            Xb = self.data[0]\n",
    "            yb = self.data[1]\n",
    "            yTy_b = self.yTy\n",
    "        else:\n",
    "            Xb = data_in[0]\n",
    "            yb = data_in[1]\n",
    "            yTy_b = tf.reduce_sum(tf.math.square(data_in[1]))\n",
    "\n",
    "        data_rotated = self.neural_net(Xb) # [N, dnn_out]\n",
    "        data_rotated = (data_rotated - tf.math.reduce_mean(data_rotated, 0)) / tf.math.reduce_std(data_rotated, 0)\n",
    "\n",
    "        inv_sigma_sq = 1 / self.sigma_n_sq\n",
    "        Lambda_Herm, V_herm = self.eigen_fun(data_rotated) # [rank, None], [N, rank]\n",
    "\n",
    "        V_lambda_sqrt = V_herm * tf.math.sqrt(Lambda_Herm) # [N, rank]\n",
    "        V_lambda_sqrt_y = tf.linalg.matvec(V_lambda_sqrt, yb, transpose_a=True) # [rank, None]\n",
    "\n",
    "        low_rank_term = self.eye_k + inv_sigma_sq * tf.linalg.matmul(V_lambda_sqrt, V_lambda_sqrt, transpose_a=True) # [rank, rank]\n",
    "        low_rank_L = tf.linalg.cholesky(low_rank_term)\n",
    "        \n",
    "        print(low_rank_L)\n",
    "        print(V_lambda_sqrt_y[:, None])\n",
    "        L_inv_V_y = tf.linalg.triangular_solve(low_rank_L, V_lambda_sqrt_y[:, None], lower=True) # [rank, 1]\n",
    "        data_fit = inv_sigma_sq * (self.yTy - inv_sigma_sq * tf.reduce_sum(tf.math.square(L_inv_V_y)))\n",
    "        return data_fit + self.num_data * tf.math.log(self.sigma_n_sq) + 2. * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(low_rank_L)))\n",
    "\n",
    "\n",
    "    def eigen_fun(self, data_x) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        beta = tf.pow(1 + 8 * self.eps_sq, .25)\n",
    "        delta_sq = 0.25 * (tf.square(beta) - 1.)\n",
    "        log_a_d_ep_sq = tf.math.log(0.5 + delta_sq + self.eps_sq)\n",
    "\n",
    "        log_term = self.dim_half * (self.minus_ln_2 - log_a_d_ep_sq) + self.sum_tupples_D * (tf.math.log(self.eps_sq) - log_a_d_ep_sq) # [rank, None]\n",
    "        gamma = tf.pow(beta, self.dim_half) * self.gamma_term # [rank, None]\n",
    "\n",
    "        tmp_exp = tf.exp(-delta_sq * ( tf.reduce_sum(tf.square(data_x), axis=1))) # [N, None]\n",
    "        x_data_upgr = beta * data_x / self.sqrt_2 # [N, D]\n",
    "\n",
    "        vander_tf = tf.math.pow(tf.expand_dims(x_data_upgr, axis=-1), self.this_range_1_2[None, :]) # [N, D, max_rank]\n",
    "        all_hermite_pols = tf.linalg.matmul(vander_tf, self.coeff_n_tf, transpose_b=True) # [N, D, max_rank]\n",
    "\n",
    "        eigen_fun_tmp = tf.map_fn(lambda x: tf.gather(all_hermite_pols[:, x], self.tupples_1[:, x], axis=-1), self.tf_range_D, dtype=default_float()) # [D, N, rank]\n",
    "        tf_lambda_n = tf.exp(log_term) # [rank, None]\n",
    "        tf_phi_n = tf.reduce_prod(eigen_fun_tmp, 0) # [N, rank]\n",
    "        tf_phi_n *= gamma # [N, rank]\n",
    "        tf_phi_n *= tmp_exp[:, None] # [N, rank]\n",
    "\n",
    "        return self.sigma_f_sq * tf_lambda_n, tf_phi_n\n",
    "\n",
    "\n",
    "    def predict_y(self, x_test: tf.Tensor, full_cov: bool = False) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        inv_sigma_sq = 1/self.sigma_n_sq\n",
    "\n",
    "        data_rotated_tr = self.neural_net(self.data[0])\n",
    "        mean_x_nn_tr, std_x_nn_tr = tf.math.reduce_mean(data_rotated_tr, 0), tf.math.reduce_std(data_rotated_tr, 0)\n",
    "        data_rotated_tr = (data_rotated_tr - mean_x_nn_tr) / std_x_nn_tr\n",
    "\n",
    "        data_rotated_tst = self.neural_net(x_test)\n",
    "        data_rotated_tst = (data_rotated_tst - mean_x_nn_tr) / std_x_nn_tr\n",
    "\n",
    "        Lambda_Herm, V_herm = self.eigen_fun(data_rotated_tr) # [N, k]\n",
    "        V_herm_test = self.eigen_fun(data_rotated_tst)[1] # [Ntest, k]\n",
    "        sqrt_Lambda_Herm = tf.math.sqrt(Lambda_Herm) # [k, None]\n",
    "\n",
    "        V_lambda_sqrt = V_herm*sqrt_Lambda_Herm # [N, k]\n",
    "        V_lambda_sqrt_y = tf.linalg.matvec(V_lambda_sqrt, self.data[1], transpose_a=True) # [k, None]\n",
    "\n",
    "        V_test_lambda_sqrt = V_herm_test*sqrt_Lambda_Herm # [Ntest, k]\n",
    "        K_Xtest_X_y =  tf.linalg.matvec(V_test_lambda_sqrt, V_lambda_sqrt_y) # [Ntest, None]\n",
    "        VT_V = tf.linalg.matmul(V_lambda_sqrt, V_lambda_sqrt, transpose_a=True) # [k, k]\n",
    "\n",
    "        low_rank_term = self.eye_k + inv_sigma_sq*VT_V # [k, k]\n",
    "        low_rank_L = tf.linalg.cholesky(low_rank_term)\n",
    "        L_inv_V_y = tf.linalg.triangular_solve(low_rank_L, V_lambda_sqrt_y[:, None], lower=True) # [k, 1]\n",
    "\n",
    "        V_K_Xtest = tf.linalg.matmul(VT_V, V_test_lambda_sqrt, transpose_b=True) # [k, N_test]\n",
    "        tmp_inv = tf.linalg.triangular_solve(low_rank_L, V_K_Xtest, lower=True) # [k, N_test]\n",
    "        mean_f = inv_sigma_sq*(K_Xtest_X_y - inv_sigma_sq*tf.linalg.matvec(tmp_inv, tf.squeeze(L_inv_V_y), transpose_a=True))\n",
    "\n",
    "        if full_cov:\n",
    "            tmp_matmul = tf.linalg.matmul(V_test_lambda_sqrt, V_K_Xtest) # [Ntest, Ntest]\n",
    "            K_Xtest_herm = tf.linalg.matmul(V_test_lambda_sqrt, V_test_lambda_sqrt, transpose_b=True) # [Ntest, Ntest]\n",
    "            var_f = K_Xtest_herm - inv_sigma_sq*(tmp_matmul - inv_sigma_sq*tf.linalg.matmul(tmp_inv, tmp_inv, transpose_a=True))\n",
    "            diag = tf.linalg.diag_part(var_f) + self.sigma_n_sq\n",
    "            var_f = tf.linalg.set_diag(var_f, diag)\n",
    "        else:\n",
    "            var_f = self.sigma_f_sq + self.sigma_n_sq - inv_sigma_sq*(tf.einsum('kn,nk->n', V_K_Xtest, V_test_lambda_sqrt) - inv_sigma_sq*tf.reduce_sum(tf.math.square(tmp_inv), 0))\n",
    "\n",
    "        return mean_f, var_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a54263dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mgp_model():     \n",
    "    print('\\n#####   MGP    #####\\n')\n",
    "    \n",
    "    # Define arrays to store values of RMSE, NLPD, and training time for each split\n",
    "    rmse_vec = np.zeros(1)\n",
    "    nlpd_vec = np.zeros(1)\n",
    "    train_time_vec = np.zeros(1)\n",
    "    \n",
    "    # for reproducibility    \n",
    "    tf.random.set_seed(1) \n",
    "    np.random.seed(1) \n",
    "\n",
    "    # Load standardized dataset\n",
    "    x_train, x_test, y_train, y_test = load_dataset_train_test_split()\n",
    "    print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    # Define model\n",
    "    model_mgp = MGP_model(data=(x_train, y_train), rank=100, dnn_out=5, eps_sq=.1, sigma_n_sq=.1)\n",
    "\n",
    "    # Split parameters to those corresponding to kernel and DNN\n",
    "    list_trainable_vars_kernel = tuple([var for var in model_mgp.trainable_variables if var.name == 'Variable:0'])\n",
    "    list_trainable_vars_dnn = tuple([var for var in model_mgp.trainable_variables if var.name != 'Variable:0'])\n",
    "    \n",
    "    # Set minibatch size\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(buffer_size=y_train.size)\n",
    "    batches_iter = iter(train_dataset.batch(batch_size))\n",
    "    iters_per_epoch = y_train.size // batch_size\n",
    "\n",
    "    # Define learning rates for kernel and DNN parameters separately\n",
    "    lrate_kernel = 0.1\n",
    "    lrate_dnn = 0.1\n",
    "\n",
    "    # Define Adam optimizers for kernel and DNN parameters separately\n",
    "    adam_opt_kernel = tf.optimizers.Adam(learning_rate=lrate_kernel)\n",
    "    adam_opt_dnn = tf.optimizers.Adam(learning_rate=lrate_dnn)\n",
    "    \n",
    "    print(list_trainable_vars_kernel)\n",
    "    print(\"\\n-----------------------------\\n\")\n",
    "    print(list_trainable_vars_dnn)\n",
    "    \n",
    "    # Differentiation of negative log marginal likelihood with respect to kernel parameters\n",
    "    @tf.function(autograph=False)\n",
    "    def optimization_step_kernel():\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(list_trainable_vars_kernel)\n",
    "            objective = model_mgp.neg_log_marginal_likelihood()\n",
    "            grads = tape.gradient(objective, list_trainable_vars_kernel)\n",
    "        adam_opt_kernel.apply_gradients(zip(grads, list_trainable_vars_kernel))\n",
    "        return objective\n",
    "\n",
    "    # Differentiation of negative log marginal likelihood with respect to DNN weights\n",
    "    @tf.function(autograph=False)\n",
    "    def optimization_step_dnn():\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(list_trainable_vars_dnn)\n",
    "            objective = model_mgp.neg_log_marginal_likelihood(next(batches_iter))\n",
    "            grads = tape.gradient(objective, list_trainable_vars_dnn)\n",
    "        adam_opt_dnn.apply_gradients(zip(grads, list_trainable_vars_dnn))\n",
    "        return objective\n",
    "    \n",
    "    print('Training...')\n",
    "    start_time = datetime.now() \n",
    "    # Training\n",
    "    for epoch in range(num_epochs):     \n",
    "        for _ in range(iters_per_epoch):\n",
    "            optimization_step_dnn()\n",
    "\n",
    "        loss_value = optimization_step_kernel()\n",
    "        if (epoch+1) % FLAGS.display_freq == 0 or epoch==0:\n",
    "            print('Epoch: {}/{}   log-marginal-lkl: {:.3f}' .format(epoch+1, num_epochs, -loss_value.numpy()))\n",
    "    train_time = datetime.now() - start_time\n",
    "    print('Done')\n",
    "\n",
    "    # Prediction\n",
    "    f_mean, f_var = model_mgp.predict_y(x_test)\n",
    "    f_mean, f_var = f_mean.numpy(), f_var.numpy()\n",
    "\n",
    "    # RMSE and NLPD computation\n",
    "    rmse_test, nlpd_test = compute_rmse_nlpd(y_test, f_mean, f_var)\n",
    "    rmse_vec[split_id], nlpd_vec[split_id], train_time_vec[split_id] = rmse_test, nlpd_test, train_time.total_seconds()\n",
    "\n",
    "    print('\\nMGP  RMSE: %.3f    NLPD: %.3f' % (rmse_test, nlpd_test))  \n",
    "    print('Training time  Full: {}  or {:.3f} seconds' .format(train_time, train_time.total_seconds()))\n",
    "            \n",
    "    print('\\n\\nDMGP-Averaged   RMSE: %.3f +/- %.3f   NLPD: %.3f +/- %.3f' % (rmse_vec.mean(), rmse_vec.std(), nlpd_vec.mean(), nlpd_vec.std()))  \n",
    "    print('Average training time  %.3f +/- %.3f seconds' %(train_time_vec.mean(), train_time_vec.std()))\n",
    "    print()\n",
    "    print(60*'=')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e887fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####   MGP    #####\n",
      "\n",
      "(80000, 27) (20000, 27) (80000, 1) (20000, 1)\n",
      "(<tf.Variable 'Variable:0' shape=(5, 5) dtype=float64, numpy=\n",
      "array([[1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1.]])>,)\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "(<tf.Variable 'softplus:0' shape=() dtype=float64, numpy=-2.2521684610440906>, <tf.Variable 'softplus:0' shape=() dtype=float64, numpy=-2.2521684610440906>, <tf.Variable 'dense_3/kernel:0' shape=(27, 5) dtype=float64, numpy=\n",
      "array([[-0.15497747, -0.39162227, -0.14245017,  0.26524996, -0.18981956],\n",
      "       [-0.35242672, -0.33369734, -0.21148278,  0.02338321, -0.35525571],\n",
      "       [-0.17471755,  0.05954031, -0.42481883,  0.23541848, -0.37755901],\n",
      "       [-0.19044198,  0.40831117, -0.14595551, -0.13869812, -0.12329508],\n",
      "       [ 0.1981159 , -0.21015189,  0.19453441, -0.43216321, -0.34893474],\n",
      "       [ 0.0573024 ,  0.3396072 ,  0.35539239, -0.02755926,  0.16253759],\n",
      "       [-0.26140044, -0.10988473, -0.33187367, -0.17918024, -0.03874302],\n",
      "       [ 0.21677573, -0.04453575,  0.18819007,  0.02835441,  0.22817411],\n",
      "       [ 0.20725624, -0.123428  ,  0.40276223, -0.18932241,  0.21612557],\n",
      "       [-0.22879877, -0.3933409 , -0.21595432, -0.04150254, -0.16082178],\n",
      "       [ 0.12667272, -0.21489819,  0.31120163, -0.22894835,  0.41093786],\n",
      "       [-0.00335917, -0.23826859,  0.10106406,  0.12089632,  0.4004914 ],\n",
      "       [-0.13062629,  0.0802433 ,  0.28002395,  0.0190133 , -0.42897663],\n",
      "       [-0.39866691,  0.29579987,  0.41122375,  0.15331098,  0.41106283],\n",
      "       [ 0.2253193 ,  0.11437868,  0.30035775,  0.07429976,  0.17679482],\n",
      "       [ 0.0940328 , -0.0691796 ,  0.40612172,  0.39704877, -0.07468499],\n",
      "       [-0.04234952,  0.35412735, -0.0812944 ,  0.35930797, -0.00386275],\n",
      "       [ 0.36741421,  0.29991995, -0.16859276, -0.35865556,  0.42216492],\n",
      "       [ 0.215993  ,  0.03248523,  0.24832463, -0.22574596,  0.05242776],\n",
      "       [ 0.33882239,  0.30516183,  0.01618896,  0.28727244, -0.20857154],\n",
      "       [ 0.19819823,  0.17058496,  0.05542717,  0.38374223, -0.07831758],\n",
      "       [ 0.370813  ,  0.31261699,  0.11909062, -0.28434314, -0.10728248],\n",
      "       [-0.33479594, -0.43005216,  0.28527027, -0.2193727 , -0.31578792],\n",
      "       [ 0.00347105,  0.20619726, -0.04300068, -0.15852167, -0.31748305],\n",
      "       [ 0.12017345,  0.42036404, -0.38046398,  0.19964092,  0.39695639],\n",
      "       [-0.42855705, -0.32769966, -0.05150229, -0.1097377 ,  0.30059452],\n",
      "       [ 0.28458477, -0.36630092, -0.29520758,  0.3072967 ,  0.0423649 ]])>, <tf.Variable 'dense_3/bias:0' shape=(5,) dtype=float64, numpy=array([0., 0., 0., 0., 0.])>)\n",
      "Training...\n",
      "Tensor(\"Cholesky:0\", shape=(100, 100), dtype=float64)\n",
      "Tensor(\"strided_slice_2:0\", shape=(None, 1, 100), dtype=float64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 100 and 1 for '{{node triangular_solve/MatrixTriangularSolve}} = MatrixTriangularSolve[T=DT_DOUBLE, adjoint=false, lower=true](Cholesky, strided_slice_3)' with input shapes: [100,100], [?,1,100].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-3ec8d7ce6602>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_mgp_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-5f4f5ca9ceca>\u001b[0m in \u001b[0;36mrun_mgp_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miters_per_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0moptimization_step_dnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimization_step_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-5f4f5ca9ceca>\u001b[0m in \u001b[0;36moptimization_step_dnn\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwatch_accessed_variables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_trainable_vars_dnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_mgp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneg_log_marginal_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_trainable_vars_dnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0madam_opt_dnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_trainable_vars_dnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-6507dbb11ae9>\u001b[0m in \u001b[0;36mneg_log_marginal_likelihood\u001b[1;34m(self, data_in)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlow_rank_L\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV_lambda_sqrt_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mL_inv_V_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtriangular_solve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlow_rank_L\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV_lambda_sqrt_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# [rank, 1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[0mdata_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minv_sigma_sq\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myTy\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0minv_sigma_sq\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL_inv_V_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata_fit\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_data\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigma_n_sq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2.\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag_part\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlow_rank_L\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 100 and 1 for '{{node triangular_solve/MatrixTriangularSolve}} = MatrixTriangularSolve[T=DT_DOUBLE, adjoint=false, lower=true](Cholesky, strided_slice_3)' with input shapes: [100,100], [?,1,100]."
     ]
    }
   ],
   "source": [
    "run_mgp_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "22ae54a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-d0221bc96ae3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Set minibatch size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbatches_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0miters_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mFLAGS_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7affff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
