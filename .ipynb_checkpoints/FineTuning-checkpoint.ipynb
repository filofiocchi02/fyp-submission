{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kq0L8j9VkUf_"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, Matern, RationalQuadratic, ExpSineSquared\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from math import sqrt\n",
    "\n",
    "from tensorflow.keras.layers import TimeDistributed, Attention, Input, Conv1D, MaxPooling1D, LSTM, Dense, Flatten, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Fv-KvXtgkVDF"
   },
   "outputs": [],
   "source": [
    "def plot_means_variances(y_true, y_means, y_stddevs):\n",
    "    plt.rc('font', size=14)\n",
    "    min_vals = np.min([np.min(y_true), np.min(y_means)])\n",
    "    max_vals = np.max([np.max(y_true), np.max(y_means)])\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    # Plot predicted vs true\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_true, y_means, alpha = .7, color=\"0.3\", linewidth = 0, s = 2)\n",
    "    plt.plot([min_vals, max_vals], [min_vals, max_vals], 'k--', color='red')  # Add diagonal line\n",
    "    plt.title('Fig (a): Predicted vs True Values')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "\n",
    "    def plot_binned_residuals(y_true, residuals, num_bins=20):\n",
    "        bins = np.linspace(min(y_true), max(y_true), num_bins + 1)\n",
    "\n",
    "        bin_means = [0]*num_bins\n",
    "        bin_stddevs = [0]*num_bins\n",
    "\n",
    "        for i in range(num_bins):\n",
    "            mask = (y_true >= bins[i]) & (y_true < bins[i + 1])\n",
    "            if np.any(mask):\n",
    "                bin_means[i] = np.mean(y_true[mask])\n",
    "                bin_stddevs[i] = np.sqrt(mean_squared_error(y_means[mask], y_true[mask]))\n",
    "        return bin_means, bin_stddevs\n",
    "\n",
    "    bin_means, bin_stddevs = plot_binned_residuals(y_true, y_means, num_bins=20)\n",
    "\n",
    "    # Plot residuals vs true\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_true, y_stddevs, alpha = .7, color=\"0.3\", linewidth = 0, s = 2, label='Predicted Standard Deviation', zorder=1)\n",
    "    plt.scatter(bin_means, bin_stddevs, alpha=1, s=50, color='red', label='True Binned Root Mean Squared Error', zorder=2)\n",
    "    plt.title('Fig (b): Predicted Standard Deviation vs True RMSE')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Standard Deviation')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_and_print_metrics(results, model_name, y_train, y_test, y_train_pred, y_test_pred, y_train_stddevs, y_test_stddevs, ci):\n",
    "    z_value = stats.norm.ppf((1 + ci) / 2)\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    train_lower_bound = y_train_pred - z_value * y_train_stddevs\n",
    "    train_upper_bound = y_train_pred + z_value * y_train_stddevs\n",
    "\n",
    "    test_lower_bound = y_test_pred - z_value * y_test_stddevs\n",
    "    test_upper_bound = y_test_pred + z_value * y_test_stddevs\n",
    "\n",
    "    train_within_interval = np.sum(np.logical_and(y_train.ravel() >= train_lower_bound, y_train.ravel() <= train_upper_bound))\n",
    "    test_within_interval = np.sum(np.logical_and(y_test.ravel() >= test_lower_bound, y_test.ravel() <= test_upper_bound))\n",
    "\n",
    "    train_percentage_within_interval = (train_within_interval / len(y_train.ravel())) * 100\n",
    "    test_percentage_within_interval = (test_within_interval / len(y_test.ravel())) * 100\n",
    "\n",
    "\n",
    "    results[model_name] = {\n",
    "        \"Test Root Mean Squared Error (RME): \": test_rmse,\n",
    "        \"Test Mean Absolute Error (MAE): \": test_mae,\n",
    "        f\"Percentage of Test Data Points within {ci:.2f}% CI: \": test_percentage_within_interval\n",
    "    }\n",
    "\n",
    "    print(f\"Train RMSE: {train_rmse:.3f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.3f}\")\n",
    "    print(f\"Train MAE: {train_mae:.3f}\")\n",
    "    print(f\"Test MAE: {test_mae:.3f}\")\n",
    "    print(f\"Percentage of Train Data Points within {ci*100:.2f}% CI: {train_percentage_within_interval:.2f}%\")\n",
    "    print(f\"Percentage of Test Data Points within {ci*100:.2f}% CI: {test_percentage_within_interval:.2f}%\")\n",
    "\n",
    "def plot_confidence_interval_histogram(y_test_pred, y_test_std, y_test, bins=20):\n",
    "    plt.rc('font', size=14)\n",
    "\n",
    "    # Compute the t-values of the confidence intervals based on Z-scores\n",
    "    t_values = np.array([norm.ppf(i/bins + (1-i/bins)/2) for i in range(1, bins+1)])\n",
    "\n",
    "    percentages_within_interval = []\n",
    "    for t_value in t_values:\n",
    "        lower_bounds = y_test_pred.ravel() - t_value * y_test_std\n",
    "        upper_bounds = y_test_pred.ravel() + t_value * y_test_std\n",
    "\n",
    "        # Count number of data points within the confidence interval\n",
    "        is_within_interval = np.logical_and(y_test >= lower_bounds, y_test <= upper_bounds)\n",
    "        num_within_interval = np.sum(is_within_interval)\n",
    "\n",
    "        # Calculate the percentage of data points within the confidence interval\n",
    "        percentage_within_interval = (num_within_interval / len(y_test)) * 100\n",
    "        percentages_within_interval.append(percentage_within_interval)\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    bars = plt.bar(np.arange(1, bins+1), percentages_within_interval, color='lightgray', edgecolor='black')\n",
    "    plt.xlabel('Confidence Intervals')\n",
    "    plt.ylabel('Percentage within Interval')\n",
    "    plt.title('Percentage of Data Points within Confidence Intervals')\n",
    "\n",
    "    # Add numbers on top of interval bar\n",
    "    for bar, percentage in zip(bars, percentages_within_interval):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 8,\n",
    "             bar.get_height() + 1,\n",
    "             f'{percentage:.1f}%',\n",
    "             fontsize=9)\n",
    "\n",
    "    plt.xticks(np.arange(1, bins+1), [f'{i/bins*100:.0f}%' for i in range(1, bins+1)])\n",
    "    plt.savefig(\"bar2.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def load_dataset_train_test_split(df, features, output_feature):\n",
    "    X = df[features]\n",
    "    y = df[output_feature]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "    # Scale input data to facilitate training\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, np.array(y_train), np.array(y_test), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QRX2mdorkaEq"
   },
   "outputs": [],
   "source": [
    "# In order to ensure that each model has repeatable results,we fix the seed both for the\n",
    "# data splitting part and for the initilialization of the networks' weights. Theoretially\n",
    "# speaking, we should average over different seeds to ensure the robustness of our results.\n",
    "# However, in practice, due to the size of the data set this is unfeasibile and we only do\n",
    "# this for the best performing model to show that the variability of results based on seed\n",
    "# is almost none.\n",
    "\n",
    "keras.utils.set_random_seed(812)\n",
    "MODELS_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dlvvb7ogkaHg",
    "outputId": "dc9d5fea-16e6-4e87-e90a-95d7f08883d5"
   },
   "outputs": [],
   "source": [
    "file_path = 'Cleaned_data.pkl'\n",
    "df_full = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JeBsqIFNkVHq",
    "outputId": "198715d0-e97a-4b6f-96eb-0cf0e5f25156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points before removing NaNs:  1018494\n",
      "Total data points after removing NaNs:  1009707\n",
      "Total data points for turbine 5 before removing NaNs:  191579\n",
      "Total data points for turbine 5 after removing NaNs:  189381\n"
     ]
    }
   ],
   "source": [
    "# Datetime column\n",
    "DATETIME_COL = 'Date.time'\n",
    "\n",
    "# Features considered\n",
    "features = [\n",
    "'Wind.speed.me',\n",
    "'Wind.speed.sd',\n",
    "'Wind.speed.min',\n",
    "'Wind.speed.max',\n",
    "'Front.bearing.temp.me',\n",
    "'Front.bearing.temp.sd',\n",
    "'Front.bearing.temp.min',\n",
    "'Front.bearing.temp.max',\n",
    "'Rear.bearing.temp.me',\n",
    "'Rear.bearing.temp.sd',\n",
    "'Rear.bearing.temp.min',\n",
    "'Rear.bearing.temp.max',\n",
    "'Rotor.bearing.temp.me',\n",
    "'Stator1.temp.me',\n",
    "'Nacelle.ambient.temp.me',\n",
    "'Nacelle.temp.me',\n",
    "'Transformer.temp.me',\n",
    "'Gear.oil.temp.me',\n",
    "'Gear.oil.temp.me.1',\n",
    "'Top.box.temp.me',\n",
    "'Hub.temp.me',\n",
    "'Conv.Amb.temp.me',\n",
    "'Rotor.bearing.temp.me',\n",
    "'Transformer.cell.temp.me',\n",
    "'Motor.axis1.temp.me',\n",
    "'Motor.axis2.temp.me',\n",
    "'CPU.temp.me',\n",
    "'Blade.ang.pitch.pos.A.me',\n",
    "'Blade.ang.pitch.pos.B.me',\n",
    "'Blade.ang.pitch.pos.C.me',\n",
    "'Gear.oil.inlet.press.me',\n",
    "'Gear.oil.pump.press.me',\n",
    "'Drive.train.acceleration.me',\n",
    "'Tower.Acceleration.x',\n",
    "'Tower.Acceleration.y'\n",
    "]\n",
    "\n",
    "output_feature = 'Power.me'\n",
    "\n",
    "df = df_full\n",
    "print(f\"Total data points before removing NaNs: \", len(df))\n",
    "df = df.dropna(subset=features + [output_feature] + [DATETIME_COL])\n",
    "print(f\"Total data points after removing NaNs: \", len(df))\n",
    "df = df.reset_index(drop=False)\n",
    "\n",
    "# Only consider the turbine with ID 5\n",
    "TURBINE_ID = 5\n",
    "df_single_turbine = df_full[df_full['turbine'] == TURBINE_ID]\n",
    "print(f\"Total data points for turbine {TURBINE_ID} before removing NaNs: \", len(df_single_turbine))\n",
    "df_single_turbine = df_single_turbine.dropna(subset=features + [output_feature] + [DATETIME_COL])\n",
    "print(f\"Total data points for turbine {TURBINE_ID} after removing NaNs: \", len(df_single_turbine))\n",
    "df_single_turbine = df_single_turbine.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ggXjEvVhTxmL"
   },
   "outputs": [],
   "source": [
    "X_train_full, X_test_full, y_train_full, y_test_full, scaler_full = load_dataset_train_test_split(df, features, output_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "smX6GWENUGr6"
   },
   "outputs": [],
   "source": [
    "X_train_single_turbine, X_test_single_turbine, y_train_single_turbine, y_test_single_turbine, scaler_single_turbine = load_dataset_train_test_split(df_single_turbine, features, output_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VjdDMFc2kVJ0"
   },
   "outputs": [],
   "source": [
    "def normal_softplus(params):\n",
    "    return tfd.Normal(loc=params[:, 0:1], scale=1e-3 + tf.math.softplus(0.05 * params[:, 1:2]))\n",
    "\n",
    "def NLL(y, distr):\n",
    "    return -distr.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0KU-4RekVMJ",
    "outputId": "f2c22fb4-9b63-4bd5-dbd0-fdd08751bd9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 35)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               18000     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 300)               150300    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 202       \n",
      "                                                                 \n",
      " distribution_lambda (Distr  ((None, 1),               0         \n",
      " ibutionLambda)               (None, 1))                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 198602 (775.79 KB)\n",
      "Trainable params: 198602 (775.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for saving the weights\n",
    "checkpoint_path = '/content/drive/My Drive/Colab Notebooks/initial_model_weights.h5'\n",
    "\n",
    "# Define the initial model architecture\n",
    "def generic_model(X_train_full):\n",
    "    inputs = Input(shape=(X_train_full.shape[1],))\n",
    "    hidden1 = Dense(500, activation=\"relu\")(inputs)\n",
    "    hidden2 = Dense(300, activation=\"relu\")(hidden1)\n",
    "    hidden3 = Dense(100, activation=\"relu\")(hidden2)\n",
    "\n",
    "    params = Dense(2)(hidden3)\n",
    "\n",
    "    dist = tfp.layers.DistributionLambda(normal_softplus)(params)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=dist)\n",
    "    model.compile(Adam(learning_rate=0.001), loss=NLL)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the initial model using X_full with the checkpoint callback\n",
    "generic_model = generic_model(X_train_full)\n",
    "generic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWkFe9AHYFIw",
    "outputId": "a90f5f30-bef8-4ab3-f920-761a7aae2fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "11360/11360 [==============================] - 69s 6ms/step - loss: 3.6434 - val_loss: 8.8135\n",
      "Epoch 2/500\n",
      "11360/11360 [==============================] - 68s 6ms/step - loss: 3.6431 - val_loss: 3.7434\n",
      "Epoch 3/500\n",
      "11360/11360 [==============================] - 69s 6ms/step - loss: 3.6490 - val_loss: 3.7598\n",
      "Epoch 4/500\n",
      "11360/11360 [==============================] - 68s 6ms/step - loss: 3.6397 - val_loss: 3.7406\n",
      "Epoch 5/500\n",
      "11360/11360 [==============================] - 65s 6ms/step - loss: 3.8042 - val_loss: 3.7348\n",
      "Epoch 6/500\n",
      "11360/11360 [==============================] - 66s 6ms/step - loss: 3.6700 - val_loss: 3.7558\n",
      "Epoch 7/500\n",
      "11360/11360 [==============================] - 72s 6ms/step - loss: 3.6394 - val_loss: 3.7085\n",
      "Epoch 8/500\n",
      "11360/11360 [==============================] - 57s 5ms/step - loss: 3.6375 - val_loss: 3.7218\n",
      "Epoch 9/500\n",
      "11360/11360 [==============================] - 63s 6ms/step - loss: 3.6376 - val_loss: 3.7726\n",
      "Epoch 10/500\n",
      "11360/11360 [==============================] - 67s 6ms/step - loss: 3.6331 - val_loss: 3.7522\n",
      "Epoch 11/500\n",
      "11360/11360 [==============================] - 68s 6ms/step - loss: 6.7026 - val_loss: 3.7778\n",
      "Epoch 12/500\n",
      "11360/11360 [==============================] - 64s 6ms/step - loss: 90635.8438 - val_loss: 3.7507\n",
      "Epoch 13/500\n",
      "11360/11360 [==============================] - 49s 4ms/step - loss: 3.6430 - val_loss: 3.8216\n",
      "Epoch 14/500\n",
      "11360/11360 [==============================] - 50s 4ms/step - loss: 3.6355 - val_loss: 3.7211\n",
      "Epoch 15/500\n",
      "11360/11360 [==============================] - 49s 4ms/step - loss: 3.6359 - val_loss: 3.7272\n",
      "Epoch 16/500\n",
      "11360/11360 [==============================] - 52s 5ms/step - loss: 3.6335 - val_loss: 3.7192\n",
      "Epoch 17/500\n",
      "11360/11360 [==============================] - 53s 5ms/step - loss: 3.6336 - val_loss: 3.7204\n",
      "Epoch 18/500\n",
      "11360/11360 [==============================] - 50s 4ms/step - loss: 3.6354 - val_loss: 3.7765\n",
      "Epoch 19/500\n",
      "11360/11360 [==============================] - 49s 4ms/step - loss: 3.6310 - val_loss: 3.7403\n",
      "Epoch 20/500\n",
      "11360/11360 [==============================] - 50s 4ms/step - loss: 3.6253 - val_loss: 3.7516\n",
      "Epoch 21/500\n",
      "11360/11360 [==============================] - 47s 4ms/step - loss: 4.1027 - val_loss: 3.7336\n",
      "Epoch 22/500\n",
      "11360/11360 [==============================] - 46s 4ms/step - loss: 3.7455 - val_loss: 3.7505\n",
      "Epoch 23/500\n",
      "11360/11360 [==============================] - 49s 4ms/step - loss: 3.6436 - val_loss: 3.7139\n",
      "Epoch 24/500\n",
      "10569/11360 [==========================>...] - ETA: 3s - loss: 3.6281"
     ]
    }
   ],
   "source": [
    "# Define the callback to save the weights\n",
    "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True,\n",
    "                                      monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "history = generic_model.fit(X_train_full, y_train_full, epochs=500, batch_size=64,\n",
    "                            validation_split=0.1, callbacks=[checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XBFsTIV1Sesa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6311/6311 [==============================] - 20s 3ms/step - loss: 3.9860\n",
      "Evaluation Loss: 3.986027479171753\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load the weights of the initial model\n",
    "generic_model.load_weights(checkpoint_path)\n",
    "evaluation = generic_model.evaluate(X_test_full, y_test_full)\n",
    "print(\"Evaluation Loss:\", evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lR7EWgab1Td"
   },
   "outputs": [],
   "source": [
    "hidden_layer = generic_model.layers[-2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_jmUfLfSZ9L"
   },
   "outputs": [],
   "source": [
    "def create_model_finetune(X_train, generic_model, seed):\n",
    "    keras.utils.set_random_seed(seed)\n",
    "\n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "    # Step 1: Load the architecture and weights of the previously trained model\n",
    "    pretrained_model_layers = generic_model.layers[1:-2]\n",
    "    x = inputs\n",
    "\n",
    "    for layer in pretrained_model_layers:\n",
    "        layer.trainable = False\n",
    "        x = layer(x)\n",
    "\n",
    "    hidden1 = Dense(100, activation=\"relu\")(x)\n",
    "    hidden2 = Dense(80, activation=\"relu\")(hidden1)\n",
    "    hidden3 = Dense(40, activation=\"relu\")(hidden2)\n",
    "\n",
    "    mean_h1 = Dense(20, activation=\"relu\")(hidden3)\n",
    "    mean_h2 = Dense(10, activation=\"relu\")(mean_h1)\n",
    "    mean_out = Dense(1)(mean_h2)\n",
    "\n",
    "    variance_h1 = Dense(20, activation=\"relu\")(hidden3)\n",
    "    variance_h2 = Dense(10, activation=\"relu\")(variance_h1)\n",
    "    variance_out = Dense(1)(variance_h2)\n",
    "\n",
    "    params = Dense(2)(Concatenate()([mean_out, variance_out]))\n",
    "\n",
    "\n",
    "    dist = tfp.layers.DistributionLambda(normal_softplus)(params)\n",
    "\n",
    "    model_mlp_gaussian = Model(inputs=inputs, outputs=dist)\n",
    "    model_mlp_gaussian.compile(Adam(learning_rate=0.001), loss=NLL)\n",
    "\n",
    "    return model_mlp_gaussian\n",
    "\n",
    "model_finetune = create_model_finetune(X_train_single_turbine, generic_model, MODELS_SEED)\n",
    "model_finetune.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtnppfnFS-WK"
   },
   "outputs": [],
   "source": [
    "# Define the callback to save the weights\n",
    "checkpoint_callback = ModelCheckpoint(filepath='/content/drive/My Drive/Colab Notebooks/final_model_weights.h5', save_weights_only=True,\n",
    "                                      monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "history = model_finetune.fit(X_train_single_turbine, y_train_single_turbine, epochs=200, batch_size=32,\n",
    "                            validation_split=0.1, callbacks=[checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqZGOme7S-bb"
   },
   "outputs": [],
   "source": [
    "y_train_pred = np.array(model_finetune(X_train_single_turbine).mean()).ravel()\n",
    "y_test_pred = np.array(model_finetune(X_test_single_turbine).mean()).ravel()\n",
    "\n",
    "y_train_stddevs = np.array(model_finetune(X_train_single_turbine).stddev()).ravel()\n",
    "y_test_stddevs = np.array(model_finetune(X_test_single_turbine).stddev()).ravel()\n",
    "\n",
    "evaluate_and_print_metrics({}, f\"Fine Tuned\",\n",
    "y_train_single_turbine, y_test_single_turbine, y_train_pred, y_test_pred,\n",
    "y_train_stddevs, y_test_stddevs, 0.99)\n",
    "\n",
    "plot_means_variances(y_test_single_turbine, y_test_pred, y_test_stddevs)\n",
    "plot_confidence_interval_histogram(y_test_pred, y_test_stddevs, y_test_single_turbine, bins=20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
